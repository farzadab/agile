TODO PPO:
    alg improvements:
        ✔ normalization (states/reward) @started(18-06-14 11:57) @done(18-06-14 14:49) @lasted(2h52m39s)
            ✔ running average @done(18-06-14 14:49)
                ✔ try naive normalization as well and run an experiment @started(18-06-14 11:57) @done(18-06-14 14:49) @lasted(2h52m41s)
                normalization of reward should help with TD estimates
                ☐ do running average but stop at some point?
            ☐ not sure if I need it for actions or not (Glen says no)
        ✔ more episodes per iteration @started(18-06-14 11:57) @done(18-06-14 14:49) @lasted(2h52m53s)
            ✔ fix ReplayMemory @critical @done(18-06-14 14:49)
        ☐ use TD estimates
            ☐ TD(1)
            ☐ TD(lambda)
    
    experiments:
        ✔ add images to tensorboard @done(18-06-18 11:50)
        ✔ better setup for running experiments @started(18-06-18 12:08) @done(18-06-18 13:23) @lasted(1h15m24s)
            ☐ validation criteria: reached as extra dict?
            ☐ register environments
        ☐ longer control step: do the same action 5-10 times
        ☐ v^2 reward: use square of the inwards velocity
        ☐ try negative distance squared again (without the bonus at the end)
        new env:
            ✔ moving target: circle @done(18-06-18 13:52)
                ☐ experiment with the effect of using phase
    
    technical:
        ✔ refactor the visualization part of the code @started(18-06-13 19:40) @done(18-06-14 14:49) @lasted(19h9m35s)
        ☐ experiment: start from the same state?
        ☐ better logging system
        ✔ store policy/value function @done(18-06-18 17:24)
            ✔ replay for policy @done(18-06-18 18:23)
        ☐ state object?

    doc:
        ☐ write up results in Github or wiki (which one?)
    
    parallelism:
        ☐ check to see if you can improve the policy update in Zhaoming's code (read https://arxiv.org/abs/1707.02286)

    ✔ implement PPO @done(18-06-13 19:40)
        ✔ read the paper! @done(18-06-08 10:32)
            ✔ ask Zhaoming if the gradient is always zero when min is clip @done(18-06-08 10:32): yes, he says
        ✔ implement a basic high-level architecture @done(18-06-11 12:33)
        ✔ take a look at Zhaoming's implementation @done(18-06-11 18:34)
        ☐ take a look at RL-Lab's implementation
        ✔ create simple environment @done(18-06-11 18:35): created PointMass env
        ✔ debug code @done(18-06-13 19:27)
    
    high-level:
        ☐ re-implement DeepMimic
            ✔ read the paper! @started(18-06-07 10:25) @done(18-06-07 14:00) @lasted(3h35m40s)
                ☐ complete wiki

Papers:
    ✔ Deepmimic https://arxiv.org/pdf/1804.02717.pdf @done(18-06-07 15:18)
    ☐ Proximal Policy Optimization (PPO) https://arxiv.org/pdf/1707.06347.pdf
    ☐ backpropagation through the void https://arxiv.org/pdf/1711.00123
    ☐ Generative advantage estimation (GAE) https://arxiv.org/pdf/1506.02438.pdf

TODO OLD:
    ☐ get good motions
        ☐ fix one, drive sinusoid from other
        ☐ A -> B with cos interpolation
        ☐ whole sinusoid: target = A_i cos(w t + phi_i)
    ☐ fitted value iteration: http://www.cs.ubc.ca/~van/papers/2009-TOG-taskControl/index.html

    ☐ simple periodic PD controls
    ✔ evaluate dynamics on longer horizons ... ? @started(18-06-06 10:57) @done(18-06-08 10:32) @lasted(1d23h35m17s)
    ☐ send Michiel the specs of computer
        GeForce GTX 285 (Last Price Change: 2011-06-23cpu $179.99 USD)
        Intel Core i5 750 (found an article from September 06, 2010)
    ✔ ask Zhaoming about the MuJoCo error @done(18-06-05 11:13): had no idea
      >>> import gym, mujoco_py
      >>> gym.make('HalfCheetahEnv-v2')
      >>> gym.make('HalfCheetah-v2')
      [1]    31083 illegal hardware instruction (core dumped)  python


    ☐ experiment with half-cheetah
    ✔ check pytorch optimization issues @critical @done(18-06-01 18:07)
        ✔ is it converging? @done(18-06-01 18:07)
        ✔ normalization @started(18-05-31 17:34) @done(18-06-01 18:07) @lasted(1d33m1s)
        ✔ correct optimization algorithm and batch size @started(18-05-31 17:34) @done(18-06-01 18:07) @lasted(1d33m2s)
            ✔ hyper-parameters @started(18-06-01 18:07) @done(18-06-04 12:58) @lasted(2d18h51m45s)
            ✔ checkout results and make more experiments @done(18-06-04 12:58)
    ☐ check the model for torque limits/mass/....
    ☐ start with a simpler model: no feet @almost
        ☐ or perhaps the model with compliant feet @almost
    ✔ add PD controller: is it going to help or not? @done(18-05-30 13:44)
    ✔ use vx for cost @done(18-05-30 15:03)
        ☐ check if cost function is working or not
    ☐ gotta get rid of "done" from the env and develop my own
        - just use timelimit for now?
    ✔ checkout cost function code from baseline paper: https://github.com/nagaban2/nn_dynamics/blob/master/reward_functions.py @done(18-06-04 12:51)
        - both this one and the homework code uses forward torso x vel + 0.05*action^2 @done(18-06-04 12:53)
        - homework code also adds some hand-engineered costs (penalizes bad configs of legs) @done(18-06-04 12:54)
    ☐ drive simple motions: fix some joints and experiment with the rest (e.g. sinosuid)
    ☐ faster MPC with multiple rollout @speed

Relevant:
    ☐ active learning?

Papers:
    ☐ Robust task-based control policies http://www.cs.ubc.ca/~van/papers/2009-TOG-taskControl/2009-TOG-taskControl.pdf
    ✔ DeepRL in a Handful of Trials using Probabilistic Dynamics https://arxiv.org/pdf/1805.12114.pdf @started(18-05-31 17:34) @done(18-06-05 19:19) @lasted(5d1h45m1s)
    ☐ World Models https://arxiv.org/pdf/1803.10122.pdf
    ☐ Bayesian Optimization Using Domain Knowledge on the ATRIAS biped https://arxiv.org/pdf/1709.06047.pdf
    ☐ Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari https://arxiv.org/abs/1802.08842
    ☐ Basic Instincts http://science.sciencemag.org/content/360/6391/845/tab-pdf
    ☐ One parameter is always enough https://colala.bcs.rochester.edu/papers/piantadosi2018one.pdf